# Author: Louis OLLIVIER (louis.xiv.bis@gmail.com)
# Date : April 2023 
import numpy as np

configfile: "config/config.yaml"

conda: "workflow/envs/environment.yaml"

# Get a list of all unique fastq files ID in the input directory
SAMPLES = []
for file in os.listdir("resources/fastq/"):
    if file.endswith(".fastq.gz"):
        SAMPLES.append(file.split(".")[0].split("_")[0])
SAMPLES = np.unique(SAMPLES)

if len(SAMPLES) < 1: 
    print("No fastq input detected")
else:
    print(f"Detected ID: {SAMPLES}")

localrules:
    all,

rule all:
    input:
        expand("results/bam/{sample}.bam", sample = SAMPLES)
        # "results/vcf/unfiltered_yeast_strains.vcf.gz",
        # "results/vcf/filtered_yeast_strains.vcf.gz",
        # "results/vcf/filtered_repremoved_yeast_strains.vcf",


##############################################################################
############# Mapping #############
# Map the fastq files to the reference genome (paired or single ends accepted)

rule bwa_map_paired:
    input:
        fastq_file_1="resources/fastq/{sample}_1.fastq.gz",
        fastq_file_2="resources/fastq/{sample}_2.fastq.gz",

    output:
        "results/sam/{sample}.sam",
    params:
        ref_genome=config["ref_genome"],
    log:
        stdout="logs/bwamem_{sample}.stdout", stderr="logs/bwamem_{sample}.stderr"
    shell:         
        "bwa mem {params.ref_genome} {input.fastq_file_1} {input.fastq_file_2} -o {output} >> {log.stdout} 2>> {log.stderr}"

rule bwa_map_single:
    input:
        "resources/fastq/{sample}.fastq.gz",
    output:
        "results/sam/{sample}.sam",
    params:
        ref_genome=config["ref_genome"],
    log:
        stdout="logs/bwamem_{sample}.stdout", stderr="logs/bwamem_{sample}.stderr"
    shell:         
        "bwa mem {params.ref_genome} {input} -o {output} >> {log.stdout} 2>> {log.stderr}"

##############################################################################
############# SAM to BAM #############
# Convert SAM files into BAM format (lighter)

rule sam_to_bam:
    input:
        "results/sam/{sample}.sam",
    output:
        "results/bam/{sample}.bam",
    params:
        ref_genome=config["ref_genome"],
    log:
        stdout="logs/samtobam_{sample}.stdout", stderr="logs/samtobam_{sample}.stderr"
    shell:         
        "samtools view -T {params.ref_genome} -Sb {input} -o {output} -O BAM >> {log.stdout} 2>> {log.stderr}"



## TODO here



############# BAM processing #############
# Sort the BAM files and index them in the first place. Then mark duplicates
# them to be ready for variant calling.


# rule sort_bam:
#     priority: 4
#     input:
#         "results/fastq_to_bam/{ENA_strain}_merged.bam",
#     output:
#         temp("results/bam/{ENA_strain}_sorted.bam"),
#     params:
#         threads=resources["sort_bam"]["cpu_tasks"],
#     threads: resources["sort_bam"]["cpu_tasks"]
#     resources:
#         slurm_partition=resources["sort_bam"]["partition"],
#         mem_mb=resources["sort_bam"]["memory"],
#         tasks=resources["sort_bam"]["tasks"],
#         cpus_per_task=resources["sort_bam"]["cpu_tasks"],
#         jobname=resources["sort_bam"]["jobname"],
#     log:
#         stdout="logs/sort_bam_{ENA_strain}.stdout", stderr="logs/sort_bam_{ENA_strain}.stderr"
#     shell:
#         "samtools sort -@ {params.threads} {input} -o {output} -O BAM > {log.stdout} 2> {log.stderr}"


# rule index_bam:
#     priority: 6
#     input:
#         "results/bam/{ENA_strain}_sorted.bam",
#     output:
#         "results/bam/{ENA_strain}_sorted.bam.bai",
#     params:
#         threads=resources["index_bam"]["cpu_tasks"],
#     threads: resources["index_bam"]["cpu_tasks"]
#     resources:
#         slurm_partition=resources["index_bam"]["partition"],
#         mem_mb=resources["index_bam"]["memory"],
#         tasks=resources["index_bam"]["tasks"],
#         cpus_per_task=resources["index_bam"]["cpu_tasks"],
#         jobname=resources["index_bam"]["jobname"],
#     log:
#         stdout="logs/index_bam_{ENA_strain}.stdout", stderr="logs/index_bam_{ENA_strain}.stderr"
#     shell:
#         "samtools index -@ {params.threads} {input} -o {output} > {log.stdout} 2> {log.stderr}"


# rule mark_duplicates:
#     priority: 8
#     input:
#         "results/bam/{ENA_strain}_sorted.bam",
#     output:
#         bam=temp("results/marked_duplicates/{ENA_strain}_sorted_marked.bam"),
#         metrics="results/metrics/MarkDuplicates/{ENA_strain}_MarkDup_metrics.txt",
#     threads: resources["mark_duplicates"]["cpu_tasks"]
#     resources:
#         slurm_partition=resources["mark_duplicates"]["partition"],
#         mem_mb=resources["mark_duplicates"]["memory"],
#         tasks=resources["mark_duplicates"]["tasks"],
#         cpus_per_task=resources["mark_duplicates"]["cpu_tasks"],
#         jobname=resources["mark_duplicates"]["jobname"],
#     log:
#         stdout="logs/mark_duplicates_{ENA_strain}.stdout", stderr="logs/mark_duplicates_{ENA_strain}.stderr"
#     shell:
#         "gatk MarkDuplicatesSpark -I {input} -O {output.bam} \
#         -M {output.metrics} --create-output-bam-index > {log.stdout} 2> {log.stderr}"


# ##############################################################################

# ############# Summary statistics of the mapping #############
# # Compute summary stats on the mapping using
# # samtools flagstats and gatk CollectAlignmentSummaryMetrics.


# rule stats_mapping:
#     input:
#         "results/marked_duplicates/{ENA_strain}_sorted_marked.bam",
#     output:
#         sam="results/metrics/flagstat/{ENA_strain}_flagstat.txt",
#         gatk="results/metrics/gatk_alig/{ENA_strain}_gatk.txt",
#     params:
#         ref_genome=config["ref_genome"],
#         threads=resources["stats_mapping"]["cpu_tasks"],
#     threads: resources["stats_mapping"]["cpu_tasks"]
#     resources:
#         slurm_partition=resources["stats_mapping"]["partition"],
#         mem_mb=resources["stats_mapping"]["memory"],
#         tasks=resources["stats_mapping"]["tasks"],
#         cpus_per_task=resources["stats_mapping"]["cpu_tasks"],
#         jobname=resources["stats_mapping"]["jobname"],
#     log:
#         stdout="logs/stats_mapping_{ENA_strain}.stdout", stderr="logs/stats_mapping_{ENA_strain}.stderr"
#     shell:
#         "samtools flagstat -@ {params.threads} -O tsv {input} > {output.sam}; \
#         gatk CollectAlignmentSummaryMetrics -R {params.ref_genome} -I {input} -O {output.gatk} > {log.stdout} 2> {log.stderr}"


# ##############################################################################
# ############# Variant calling (BAM -> gVCF -> VCF) #############
# # Variant calling for each genome, stored into per strain gVCF files
# # using gatk HaplotypeCaller.


# rule variant_calling_gvcf:
#     priority: 10
#     input:
#         "results/marked_duplicates/{ENA_strain}_sorted_marked.bam",
#     output:
#         "results/gvcf/{ENA_strain}.g.vcf.gz",  # needed if we add new samples to merge gvcf 
#     params:
#         ref_genome=config["ref_genome"],
#     threads: resources["variant_calling_gvcf"]["cpu_tasks"]
#     resources:
#         slurm_partition=resources["variant_calling_gvcf"]["partition"],
#         mem_mb=resources["variant_calling_gvcf"]["memory"],
#         tasks=resources["variant_calling_gvcf"]["tasks"],
#         cpus_per_task=resources["variant_calling_gvcf"]["cpu_tasks"],
#         jobname=resources["variant_calling_gvcf"]["jobname"],
#     log:
#         stdout="logs/variant_calling_gvcf_{ENA_strain}.stdout", stderr="logs/variant_calling_gvcf_{ENA_strain}.stderr"
#     shell:
#         "gatk HaplotypeCaller -R {params.ref_genome} -I {input} -O {output} -ERC GVCF > {log.stdout} 2> {log.stderr}"


# # Combine per sample gVCF into multi-sample gVCF using CombineGVCFs
# # (since we only produce gVCF w/ HaplotypeCaller it's OK tu use this tool).


# rule merge_gvcfs:
#     input:
#         expand("results/gvcf/{ENA_strain}.g.vcf.gz", ENA_strain=ENA_strain_list),
#     output:
#         temp("results/vcf/merged_strains.g.vcf.gz"),
#     params:
#         ref_genome=config["ref_genome"],
#         file_name="list_gvcf.list",
#     threads: resources["merge_gvcfs"]["cpu_tasks"]
#     resources:
#         slurm_partition=resources["merge_gvcfs"]["partition"],
#         mem_mb=resources["merge_gvcfs"]["memory"],
#         tasks=resources["merge_gvcfs"]["tasks"],
#         cpus_per_task=resources["merge_gvcfs"]["cpu_tasks"],
#         jobname=resources["merge_gvcfs"]["jobname"],
#     log:
#         stdout="logs/merge_gvcfs.stdout", stderr="logs/merge_gvcfs.stderr"
#     run:
#         with open(params.file_name, "w") as file:
#             for element in input:
#                 file.write(str(element) + "\n")

#         shell(
#             "gatk CombineGVCFs -R {params.ref_genome} -O {output} -V {params.file_name} > {log.stdout} 2> {log.stderr}"
#         )
#         os.remove(params.file_name)


# # Convert the multi-sample gVCF to the multi-sample VCF file
# # using GenotypeGVCFs.


# rule gvcf_to_vcf:
#     input:
#         "results/vcf/merged_strains.g.vcf.gz",
#     output:
#         "results/vcf/unfiltered_yeast_strains.vcf.gz",
#     params:
#         ref_genome=config["ref_genome"],
#     threads: resources["gvcf_to_vcf"]["cpu_tasks"]
#     resources:
#         slurm_partition=resources["gvcf_to_vcf"]["partition"],
#         mem_mb=resources["gvcf_to_vcf"]["memory"],
#         tasks=resources["gvcf_to_vcf"]["tasks"],
#         cpus_per_task=resources["gvcf_to_vcf"]["cpu_tasks"],
#         jobname=resources["gvcf_to_vcf"]["jobname"],
#     log:
#         stdout="logs/gvcf_to_vcf.stdout", stderr="logs/gvcf_to_vcf.stderr"
#     shell:
#         "gatk GenotypeGVCFs -R {params.ref_genome} -V {input} -O {output} > {log.stdout} 2> {log.stderr}"


# # Fill the filter column in the VCF: hard filtering based on previous tests (remove if TRUE)


# rule add_filter_vcf:
#     input:
#         "results/vcf/unfiltered_yeast_strains.vcf.gz",
#     output:
#         temp("results/vcf/addfilter_yeast_strains.vcf.gz"),
#     params:
#         ref_genome=config["ref_genome"],
#     threads: resources["add_filter_vcf"]["cpu_tasks"]
#     resources:
#         slurm_partition=resources["add_filter_vcf"]["partition"],
#         mem_mb=resources["add_filter_vcf"]["memory"],
#         tasks=resources["add_filter_vcf"]["tasks"],
#         cpus_per_task=resources["add_filter_vcf"]["cpu_tasks"],
#         jobname=resources["add_filter_vcf"]["jobname"],
#     log:
#         stdout="logs/add_filter_vcf.stdout", stderr="logs/add_filter_vcf.stderr"
#     shell:
#         """
#         gatk VariantFiltration -R {params.ref_genome} \
#         -V {input} \
#         -filter "QD < 10.0" --filter-name "QD10" \
#         -filter "SOR > 3.0" --filter-name "SOR3" \
#         -filter "FS > 60.0" --filter-name "FS60" \
#         -filter "MQ < 50.0" --filter-name "MQ50" \
#         -O {output} > {log.stdout} 2> {log.stderr}
#         """


# # #    "-filter 'MQRankSum < (-12.5)' --filter-name 'MQRankSum-12.5' "
# # #    "-filter 'ReadPosRankSum < (-8)' --filter-name ReadPosRankSum-8' "

# # Remove the SNP that didn't pass the filter (based on the FILTER column)


# rule filter_vcf:
#     input:
#         "results/vcf/addfilter_yeast_strains.vcf.gz",
#     output:
#         "results/vcf/filtered_yeast_strains.vcf.gz",
#     threads: resources["filter_vcf"]["cpu_tasks"]
#     resources:
#         slurm_partition=resources["filter_vcf"]["partition"],
#         mem_mb=resources["filter_vcf"]["memory"],
#         tasks=resources["filter_vcf"]["tasks"],
#         cpus_per_task=resources["filter_vcf"]["cpu_tasks"],
#         jobname=resources["filter_vcf"]["jobname"],
#     log:
#         stderr="logs/filter_vcf.stderr"
#     shell:
#         "vcftools --gzvcf {input} --remove-filtered-all --recode --stdout | gzip -c > {output} 2> {log.stderr}"

# ############# Remove repeted regions #############
# # Remove repeted regions from the genome (see resources/README.md) 
# # for more info about the input file


# rule remove_rep_regions:
#     priority: 12
#     input:
#         "results/vcf/filtered_yeast_strains.vcf.gz",
#     output:
#         "results/vcf/filtered_repremoved_yeast_strains.vcf",
#     params:
#         rep_regions=config["rep_regions"],
#     threads: resources["remove_rep_regions"]["cpu_tasks"]
#     resources:
#         slurm_partition=resources["remove_rep_regions"]["partition"],
#         mem_mb=resources["remove_rep_regions"]["memory"],
#         tasks=resources["remove_rep_regions"]["tasks"],
#         cpus_per_task=resources["remove_rep_regions"]["cpu_tasks"],
#         jobname=resources["remove_rep_regions"]["jobname"],
#     log:
#         stderr="logs/remove_rep_regions.stderr"
#     shell:
#         "bcftools view -h {input} > {output} 2> {log.stderr}; \
#         bedtools subtract -a {input} -b {params.rep_regions} >> {output} 2>> {log.stderr}"
