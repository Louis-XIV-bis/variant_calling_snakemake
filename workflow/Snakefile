# Author: Louis OLLIVIER (louis.xiv.bis@gmail.com)
# Date : February 2023 

configfile: "config/config.yaml"

# Load the predefined resource requirments ("config/resources.yaml")
import yaml, json, numpy as np, pandas as pd
with open("config/resources.yaml", "r") as f:
    resources = yaml.safe_load(f)

conda: "workflow/envs/environment.yaml"

############# Downloading, merging and filtering the information files #############
# Download all the tsv files from the ENA IDs. They contain all the information
# requiered for the rest of the pipeline (sample ID, ftp link to dl fastq, etc).
# After that, it's merged into one unique file and filtered / processed (e.g. keep
# only S.cere sequences). Then, a csv table is created for each ENA_strain ID from the table.

# Done by the Python script ran before this pipeline, we just read the list of IDs
try:   
    with open('results/table/ENA_strain_list.json', 'r') as file:
        ENA_strain_list = np.array(json.load(file))
except FileNotFoundError:
    raise FileNotFoundError("Please, make sure you ran the 'generate_table.py' script before that pipeline. If you did, the output file was not correctly created.")

localrules:
    all,


# IMPORTANT INFORMATION :
# We want to run the following steps (alignement, etc) for each individual strain of each article.
# The problem is, on some article, a strain is sequenced more than once. Then, the first steps
# (aligment, etc) will be done for each sample and then the samples of the same strain for each
# article will be merged into a single strain BAM file.
# We the the "priority" in snakemake to make sure that the pipeline will rather do the whole way
# to gVCF for a given strain (of a given article) instead of doing the mapping on all sample at
# the same time. This way, we won't have to store a large number of fastq at the same time.
# Note: most of the strains are sequenced only once per paper.


rule all:
    input:
        "results/vcf/unfiltered_yeast_strains.vcf.gz",
        "results/vcf/filtered_yeast_strains.vcf.gz",
        "results/vcf/filtered_repremoved_yeast_strains.vcf",



# In order to improve storage (but not reproducibilty), we gather the fastq dowloading, mapping and bam merge
# into the same rule. This rule does the following (in order): download fastq + check md5, mapping, sam to bam
# adding read readgroup and then merge all the bam together (since it's the same strain).


rule fastq_to_mergedbam:
    priority: 2
    input:
        table="results/table/{ENA_strain}.csv",
    output:
        merged_bam=temp("results/fastq_to_bam/{ENA_strain}_merged.bam"),
    params:
        ref_genome=config["ref_genome"],
        threads=resources["fastq_to_mergedbam"]["cpu_tasks"],
    threads: resources["fastq_to_mergedbam"]["cpu_tasks"]
    resources:
        slurm_partition=resources["fastq_to_mergedbam"]["partition"],
        mem_mb=resources["fastq_to_mergedbam"]["memory"],
        tasks=resources["fastq_to_mergedbam"]["tasks"],
        cpus_per_task=resources["fastq_to_mergedbam"]["cpu_tasks"],
        jobname=resources["fastq_to_mergedbam"]["jobname"],
    log:
        stdout="logs/fastq_to_mergedbam_{ENA_strain}.stdout", stderr="logs/fastq_to_mergedbam_{ENA_strain}.stderr"
    run:
        # Initialize temporary working directory
        tmp_wd = "./results/fastq_to_bam/"
        if not os.path.exists(tmp_wd):
            os.makedirs(tmp_wd)

        df = pd.read_csv(input.table)

        # Loop through each line of the df (= 1 run) to download and map each run individually
        # and merge the resulting files after
        for index, row in df.iterrows():
            # Download fastq & check md5 for PAIRED / SINGLE ends (easier to split that to have one on the same code)
            if row.end == "SINGLE":
                fastq_file = f"{tmp_wd}{row.run_accession}.fastq"

                # Download fastq and check if md5 is correct
                shell(
                    "workflow/scripts/dl_check_fastq.sh {fastq_file} {row.fastq_ftp} {row.fastq_md5} > {log.stdout} 2> {log.stderr}"
                )

                # Mapping of the reads and used fastq
                sam_file = f"{tmp_wd}{row.run_accession}.sam"
                shell(
                    "bwa mem {params.ref_genome} -t {params.threads} {fastq_file} -o {sam_file} >> {log.stdout} 2>> {log.stderr}"
                )
                os.remove(fastq_file)

            else:  # == PAIRED
                fastq_file_1 = f"{tmp_wd}{row.run_accession}_1.fastq"
                fastq_file_2 = f"{tmp_wd}{row.run_accession}_2.fastq"
    
                ftp_list = row.fastq_ftp.split(";")
                md5_list = row.fastq_md5.split(";")

                # Download fastq and check if md5 is correct
                shell(
                    "workflow/scripts/dl_check_fastq.sh {fastq_file_1} {ftp_list[0]} {md5_list[0]} >> {log.stdout} 2>> {log.stderr}"
                )
                shell(
                    "workflow/scripts/dl_check_fastq.sh {fastq_file_2} {ftp_list[1]} {md5_list[1]} >> {log.stdout} 2>> {log.stderr}"
                )

                # Mapping of the reads and used fastq
                sam_file = f"{tmp_wd}{row.run_accession}.sam"
                shell(
                    "bwa mem {params.ref_genome} -t {params.threads} {fastq_file_1} {fastq_file_2} -o {sam_file} >> {log.stdout} 2>> {log.stderr}"
                )
                os.remove(fastq_file_1)
                os.remove(fastq_file_2)

                # Convert SAM to BAM (both PAIRED / SINGLE end now)
            bam_file = f"{tmp_wd}{row.run_accession}.bam"
            shell(
                "samtools view -T {params.ref_genome} -Sb -@ {params.threads} {sam_file} -o {bam_file} -O BAM >> {log.stdout} 2>> {log.stderr}"
            )
            os.remove(sam_file)

            # Add read group to each BAM file
            RG_bam_file = f"{tmp_wd}{row.run_accession}_RG.bam"
            strain = str(row.ENA_strain_id.split("_")[1])
            read_group = f"@RG\tID:{row.ENA_strain_id}_{row.run_accession}\tSM:{strain}\tPL:{row.instrument_platform}"
            shell(
                "samtools addreplacerg -@ {params.threads} -r '{read_group}' {bam_file} -o {RG_bam_file} -O BAM >> {log.stdout} 2>> {log.stderr}"
            )
            os.remove(bam_file)

        # Merge and rename BAM (only rename when 1 file) then delete used files (tool needs a file with 1 bam file per line)
        merged_bam_file = output.merged_bam
        RG_bam_list = [
            f"{tmp_wd}{accession}_RG.bam" for accession in df["run_accession"]
        ]
        bam_list_file = f"{tmp_wd}{wildcards.ENA_strain}_bamlist.txt"  # file w/ list of BAM files to merge for the tool
        with open(bam_list_file, "w") as file:
            for bam in RG_bam_list:
                file.write(str(bam) + "\n")

        if len(RG_bam_list) == 1:  # no BAM to merge = rename only
            os.rename(RG_bam_list[0], merged_bam_file)
        else:  # merge all the BAM in the list
            shell(
                "samtools merge -@ {params.threads} -b {bam_list_file} {merged_bam_file} -O BAM >> {log.stdout} 2>> {log.stderr}"
            )
            for rg_bam in RG_bam_list:
                os.remove(rg_bam)
        os.remove(bam_list_file)


##############################################################################

############# BAM processing #############
# Sort the BAM files and index them in the first place. Then mark duplicates
# them to be ready for variant calling.


rule sort_bam:
    priority: 4
    input:
        "results/fastq_to_bam/{ENA_strain}_merged.bam",
    output:
        temp("results/bam/{ENA_strain}_sorted.bam"),
    params:
        threads=resources["sort_bam"]["cpu_tasks"],
    threads: resources["sort_bam"]["cpu_tasks"]
    resources:
        slurm_partition=resources["sort_bam"]["partition"],
        mem_mb=resources["sort_bam"]["memory"],
        tasks=resources["sort_bam"]["tasks"],
        cpus_per_task=resources["sort_bam"]["cpu_tasks"],
        jobname=resources["sort_bam"]["jobname"],
    log:
        stdout="logs/sort_bam_{ENA_strain}.stdout", stderr="logs/sort_bam_{ENA_strain}.stderr"
    shell:
        "samtools sort -@ {params.threads} {input} -o {output} -O BAM > {log.stdout} 2> {log.stderr}"


rule index_bam:
    priority: 6
    input:
        "results/bam/{ENA_strain}_sorted.bam",
    output:
        "results/bam/{ENA_strain}_sorted.bam.bai",
    params:
        threads=resources["index_bam"]["cpu_tasks"],
    threads: resources["index_bam"]["cpu_tasks"]
    resources:
        slurm_partition=resources["index_bam"]["partition"],
        mem_mb=resources["index_bam"]["memory"],
        tasks=resources["index_bam"]["tasks"],
        cpus_per_task=resources["index_bam"]["cpu_tasks"],
        jobname=resources["index_bam"]["jobname"],
    log:
        stdout="logs/index_bam_{ENA_strain}.stdout", stderr="logs/index_bam_{ENA_strain}.stderr"
    shell:
        "samtools index -@ {params.threads} {input} -o {output} > {log.stdout} 2> {log.stderr}"


rule mark_duplicates:
    priority: 8
    input:
        "results/bam/{ENA_strain}_sorted.bam",
    output:
        bam=temp("results/marked_duplicates/{ENA_strain}_sorted_marked.bam"),
        metrics="results/metrics/MarkDuplicates/{ENA_strain}_MarkDup_metrics.txt",
    threads: resources["mark_duplicates"]["cpu_tasks"]
    resources:
        slurm_partition=resources["mark_duplicates"]["partition"],
        mem_mb=resources["mark_duplicates"]["memory"],
        tasks=resources["mark_duplicates"]["tasks"],
        cpus_per_task=resources["mark_duplicates"]["cpu_tasks"],
        jobname=resources["mark_duplicates"]["jobname"],
    log:
        stdout="logs/mark_duplicates_{ENA_strain}.stdout", stderr="logs/mark_duplicates_{ENA_strain}.stderr"
    shell:
        "gatk MarkDuplicatesSpark -I {input} -O {output.bam} \
        -M {output.metrics} --create-output-bam-index > {log.stdout} 2> {log.stderr}"


##############################################################################

############# Summary statistics of the mapping #############
# Compute summary stats on the mapping using
# samtools flagstats and gatk CollectAlignmentSummaryMetrics.


rule stats_mapping:
    input:
        "results/marked_duplicates/{ENA_strain}_sorted_marked.bam",
    output:
        sam="results/metrics/flagstat/{ENA_strain}_flagstat.txt",
        gatk="results/metrics/gatk_alig/{ENA_strain}_gatk.txt",
    params:
        ref_genome=config["ref_genome"],
        threads=resources["stats_mapping"]["cpu_tasks"],
    threads: resources["stats_mapping"]["cpu_tasks"]
    resources:
        slurm_partition=resources["stats_mapping"]["partition"],
        mem_mb=resources["stats_mapping"]["memory"],
        tasks=resources["stats_mapping"]["tasks"],
        cpus_per_task=resources["stats_mapping"]["cpu_tasks"],
        jobname=resources["stats_mapping"]["jobname"],
    log:
        stdout="logs/stats_mapping_{ENA_strain}.stdout", stderr="logs/stats_mapping_{ENA_strain}.stderr"
    shell:
        "samtools flagstat -@ {params.threads} -O tsv {input} > {output.sam}; \
        gatk CollectAlignmentSummaryMetrics -R {params.ref_genome} -I {input} -O {output.gatk} > {log.stdout} 2> {log.stderr}"


##############################################################################
############# Variant calling (BAM -> gVCF -> VCF) #############
# Variant calling for each genome, stored into per strain gVCF files
# using gatk HaplotypeCaller.


rule variant_calling_gvcf:
    priority: 10
    input:
        "results/marked_duplicates/{ENA_strain}_sorted_marked.bam",
    output:
        "results/gvcf/{ENA_strain}.g.vcf.gz",  # needed if we add new samples to merge gvcf 
    params:
        ref_genome=config["ref_genome"],
    threads: resources["variant_calling_gvcf"]["cpu_tasks"]
    resources:
        slurm_partition=resources["variant_calling_gvcf"]["partition"],
        mem_mb=resources["variant_calling_gvcf"]["memory"],
        tasks=resources["variant_calling_gvcf"]["tasks"],
        cpus_per_task=resources["variant_calling_gvcf"]["cpu_tasks"],
        jobname=resources["variant_calling_gvcf"]["jobname"],
    log:
        stdout="logs/variant_calling_gvcf_{ENA_strain}.stdout", stderr="logs/variant_calling_gvcf_{ENA_strain}.stderr"
    shell:
        "gatk HaplotypeCaller -R {params.ref_genome} -I {input} -O {output} -ERC GVCF > {log.stdout} 2> {log.stderr}"


# Combine per sample gVCF into multi-sample gVCF using CombineGVCFs
# (since we only produce gVCF w/ HaplotypeCaller it's OK tu use this tool).


rule merge_gvcfs:
    input:
        expand("results/gvcf/{ENA_strain}.g.vcf.gz", ENA_strain=ENA_strain_list),
    output:
        temp("results/vcf/merged_strains.g.vcf.gz"),
    params:
        ref_genome=config["ref_genome"],
        file_name="list_gvcf.list",
    threads: resources["merge_gvcfs"]["cpu_tasks"]
    resources:
        slurm_partition=resources["merge_gvcfs"]["partition"],
        mem_mb=resources["merge_gvcfs"]["memory"],
        tasks=resources["merge_gvcfs"]["tasks"],
        cpus_per_task=resources["merge_gvcfs"]["cpu_tasks"],
        jobname=resources["merge_gvcfs"]["jobname"],
    log:
        stdout="logs/merge_gvcfs.stdout", stderr="logs/merge_gvcfs.stderr"
    run:
        with open(params.file_name, "w") as file:
            for element in input:
                file.write(str(element) + "\n")

        shell(
            "gatk CombineGVCFs -R {params.ref_genome} -O {output} -V {params.file_name} > {log.stdout} 2> {log.stderr}"
        )
        os.remove(params.file_name)


# Convert the multi-sample gVCF to the multi-sample VCF file
# using GenotypeGVCFs.


rule gvcf_to_vcf:
    input:
        "results/vcf/merged_strains.g.vcf.gz",
    output:
        "results/vcf/unfiltered_yeast_strains.vcf.gz",
    params:
        ref_genome=config["ref_genome"],
    threads: resources["gvcf_to_vcf"]["cpu_tasks"]
    resources:
        slurm_partition=resources["gvcf_to_vcf"]["partition"],
        mem_mb=resources["gvcf_to_vcf"]["memory"],
        tasks=resources["gvcf_to_vcf"]["tasks"],
        cpus_per_task=resources["gvcf_to_vcf"]["cpu_tasks"],
        jobname=resources["gvcf_to_vcf"]["jobname"],
    log:
        stdout="logs/gvcf_to_vcf.stdout", stderr="logs/gvcf_to_vcf.stderr"
    shell:
        "gatk GenotypeGVCFs -R {params.ref_genome} -V {input} -O {output} > {log.stdout} 2> {log.stderr}"


# Fill the filter column in the VCF: hard filtering based on previous tests (remove if TRUE)


rule add_filter_vcf:
    input:
        "results/vcf/unfiltered_yeast_strains.vcf.gz",
    output:
        temp("results/vcf/addfilter_yeast_strains.vcf.gz"),
    params:
        ref_genome=config["ref_genome"],
    threads: resources["add_filter_vcf"]["cpu_tasks"]
    resources:
        slurm_partition=resources["add_filter_vcf"]["partition"],
        mem_mb=resources["add_filter_vcf"]["memory"],
        tasks=resources["add_filter_vcf"]["tasks"],
        cpus_per_task=resources["add_filter_vcf"]["cpu_tasks"],
        jobname=resources["add_filter_vcf"]["jobname"],
    log:
        stdout="logs/add_filter_vcf.stdout", stderr="logs/add_filter_vcf.stderr"
    shell:
        """
        gatk VariantFiltration -R {params.ref_genome} \
        -V {input} \
        -filter "QD < 10.0" --filter-name "QD10" \
        -filter "SOR > 3.0" --filter-name "SOR3" \
        -filter "FS > 60.0" --filter-name "FS60" \
        -filter "MQ < 50.0" --filter-name "MQ50" \
        -O {output} > {log.stdout} 2> {log.stderr}
        """


# #    "-filter 'MQRankSum < (-12.5)' --filter-name 'MQRankSum-12.5' "
# #    "-filter 'ReadPosRankSum < (-8)' --filter-name ReadPosRankSum-8' "

# Remove the SNP that didn't pass the filter (based on the FILTER column)


rule filter_vcf:
    input:
        "results/vcf/addfilter_yeast_strains.vcf.gz",
    output:
        "results/vcf/filtered_yeast_strains.vcf.gz",
    threads: resources["filter_vcf"]["cpu_tasks"]
    resources:
        slurm_partition=resources["filter_vcf"]["partition"],
        mem_mb=resources["filter_vcf"]["memory"],
        tasks=resources["filter_vcf"]["tasks"],
        cpus_per_task=resources["filter_vcf"]["cpu_tasks"],
        jobname=resources["filter_vcf"]["jobname"],
    log:
        stderr="logs/filter_vcf.stderr"
    shell:
        "vcftools --gzvcf {input} --remove-filtered-all --recode --stdout | gzip -c > {output} 2> {log.stderr}"

############# Remove repeted regions #############
# Remove repeted regions from the genome (see resources/README.md) 
# for more info about the input file


rule remove_rep_regions:
    priority: 12
    input:
        "results/vcf/filtered_yeast_strains.vcf.gz",
    output:
        "results/vcf/filtered_repremoved_yeast_strains.vcf",
    params:
        rep_regions=config["rep_regions"],
    threads: resources["remove_rep_regions"]["cpu_tasks"]
    resources:
        slurm_partition=resources["remove_rep_regions"]["partition"],
        mem_mb=resources["remove_rep_regions"]["memory"],
        tasks=resources["remove_rep_regions"]["tasks"],
        cpus_per_task=resources["remove_rep_regions"]["cpu_tasks"],
        jobname=resources["remove_rep_regions"]["jobname"],
    log:
        stderr="logs/remove_rep_regions.stderr"
    shell:
        "bcftools view -h {input} > {output} 2> {log.stderr}; \
        bedtools subtract -a {input} -b {params.rep_regions} >> {output} 2>> {log.stderr}"
